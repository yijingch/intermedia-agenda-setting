{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/yijingch/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/yijingch/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os, re\n",
    "from datetime import datetime\n",
    "\n",
    "from src.utils.downstream_aggregate import load_model_output \n",
    "from src.utils.downstream_aggregate import aggregate_headline_wordvec, aggregate_survey_wordvec, aggregate_tweet_wordvec\n",
    "from src.utils.downstream_aggregate import bootstrap_aggregate_wordvec\n",
    "from src.utils.data_loader import Headlines, Surveys, Tweets\n",
    "from src.utils.downstream_process import clean_domain_url, trim_period\n",
    "from src.utils.dict_loader import TopicDictionary\n",
    "\n",
    "import yaml\n",
    "with open(\"../../src/configs.yml\", \"r\") as configs:\n",
    "    configs = yaml.safe_load(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input \n",
    "ROOTPATH = configs[\"ROOTPATH\"]\n",
    "DATAPATH = configs[\"DATAPATH\"] \n",
    "CACHE_FPATH = configs[\"WORDVEC_CACHE_PATH\"]\n",
    "date_string = \"033024\"\n",
    "\n",
    "# output\n",
    "AGGR_UNIT = \"D\"\n",
    "if AGGR_UNIT == \"W\":\n",
    "    OUTPUT_FPATH = configs[\"WEEK_WORDVEC_PATH\"]\n",
    "elif AGGR_UNIT == \"D\":\n",
    "    OUTPUT_FPATH = configs[\"DATE_WORDVEC_PATH\"]\n",
    "else:\n",
    "    print(\"Please enter a valid aggregation unit (W or D)!\")\n",
    "\n",
    "if not os.path.exists(OUTPUT_FPATH):\n",
    "    os.mkdir(OUTPUT_FPATH)\n",
    "    print(\"Created output folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dictionary!\n",
      "\t# of unique topics: 27\n",
      "\t# of unique words: 1426\n",
      "Successfully loaded dictionary!\n",
      "\t# of unique topics: 27\n",
      "\t# of unique words: 1453\n",
      "2020-07-01 00:00:00\n",
      "2020-11-30 00:00:00\n",
      "# of domains to include: 805\n"
     ]
    }
   ],
   "source": [
    "from src.utils.dict_configuration import dictionary2016, dictionary2020\n",
    "year = 2020\n",
    "\n",
    "if year == 2016:\n",
    "    cand1 = \"trump\"\n",
    "    cand2 = \"clinton\"\n",
    "    start = pd.to_datetime(configs[\"START2016\"])\n",
    "    end = pd.to_datetime(configs[\"END2016\"])\n",
    "    dictionary = dictionary2016\n",
    "    domains_to_include_df = pd.read_csv(\"../../index/domains/domains_to_keep2016_coverage0.5.csv\")\n",
    "    \n",
    "else:\n",
    "    cand1 = \"biden\"\n",
    "    cand2 = \"trump\"\n",
    "    start = pd.to_datetime(configs[\"START2020\"])\n",
    "    end = pd.to_datetime(configs[\"END2020\"])\n",
    "    dictionary = dictionary2020\n",
    "    domains_to_include_df = pd.read_csv(\"../../index/domains/domains_to_keep2020_coverage0.5.csv\")\n",
    "\n",
    "print(start)\n",
    "print(end)\n",
    "print(\"# of domains to include:\", len(domains_to_include_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregate headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_cachepath1 = CACHE_FPATH + f\"headline/{date_string}_{cand1}{year}_wordvec_cache.pkl\"\n",
    "headline_cachepath2 = CACHE_FPATH + f\"headline/{date_string}_{cand2}{year}_wordvec_cache.pkl\"\n",
    "headline_words1 = load_model_output(headline_cachepath1, start=start, end=end)\n",
    "headline_words2 = load_model_output(headline_cachepath2, start=start, end=end)\n",
    "\n",
    "headlines = Headlines(ROOTPATH + \"data/\", year=year, drop_duplicates=False)\n",
    "headlines.trim(start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-09 00:00:00\n",
      "2020-11-29 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# don't trim\n",
    "print(headline_words1[\"date\"].min())\n",
    "print(headline_words1[\"date\"].max())\n",
    "\n",
    "# 2017-08-09 00:00:00\n",
    "# 2020-11-29 00:00:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load domain labels --- NEW VERSION AFTER I REFRESH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>date</th>\n",
       "      <th>n_coverage</th>\n",
       "      <th>pct_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12news.com</td>\n",
       "      <td>{Timestamp('2020-09-22 00:00:00'), Timestamp('...</td>\n",
       "      <td>145</td>\n",
       "      <td>0.953947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21stcenturywire.com</td>\n",
       "      <td>{Timestamp('2020-09-22 00:00:00'), Timestamp('...</td>\n",
       "      <td>128</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4threvolutionarywar.wordpress.com</td>\n",
       "      <td>{Timestamp('2020-09-22 00:00:00'), Timestamp('...</td>\n",
       "      <td>131</td>\n",
       "      <td>0.861842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>680news.com</td>\n",
       "      <td>{Timestamp('2020-09-22 00:00:00'), Timestamp('...</td>\n",
       "      <td>108</td>\n",
       "      <td>0.710526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abc12.com</td>\n",
       "      <td>{Timestamp('2020-08-29 00:00:00'), Timestamp('...</td>\n",
       "      <td>89</td>\n",
       "      <td>0.585526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              domain  \\\n",
       "0                         12news.com   \n",
       "1                21stcenturywire.com   \n",
       "2  4threvolutionarywar.wordpress.com   \n",
       "3                        680news.com   \n",
       "4                          abc12.com   \n",
       "\n",
       "                                                date  n_coverage  pct_coverage  \n",
       "0  {Timestamp('2020-09-22 00:00:00'), Timestamp('...         145      0.953947  \n",
       "1  {Timestamp('2020-09-22 00:00:00'), Timestamp('...         128      0.842105  \n",
       "2  {Timestamp('2020-09-22 00:00:00'), Timestamp('...         131      0.861842  \n",
       "3  {Timestamp('2020-09-22 00:00:00'), Timestamp('...         108      0.710526  \n",
       "4  {Timestamp('2020-08-29 00:00:00'), Timestamp('...          89      0.585526  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_to_include_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df = pd.read_csv(\"../../index/domains/MASTER_fake_refreshed081123.tsv\", sep=\"\\t\")\n",
    "mixed_df = pd.read_csv(\"../../index/domains/MASTER_mixed_refreshed081123.tsv\", sep=\"\\t\")\n",
    "ideo_df = pd.read_csv(\"../../index/domains/ideo_domain_mbfc081123.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of domains: 805\n",
      "# of right-leaning domains: 233\n",
      "# of center domains: 134\n",
      "# of left-leaning domains: 249\n",
      "# of low-credibility domains: 221\n",
      "# of traditional domains: 505\n"
     ]
    }
   ],
   "source": [
    "domains_to_include = domains_to_include_df[\"domain\"].tolist()\n",
    "lowcs = set(domains_to_include).intersection(set(fake_df[fake_df[\"fake_sum\"]>=1][\"domain\"].tolist()))\n",
    "lowcs.remove(\"foxnews.com\")\n",
    "\n",
    "trads = set(domains_to_include) - set(fake_df[fake_df[\"fake_sum\"]>=1][\"domain\"].tolist())\n",
    "trads -= set(mixed_df[mixed_df[\"mixed_sum\"]>=1][\"domain\"].tolist())\n",
    "trads.add(\"foxnews.com\")\n",
    "\n",
    "lefts = set(domains_to_include).intersection(set(ideo_df[ideo_df[\"mbfc_ideo\"]==-1][\"domain\"].tolist()))\n",
    "rights = set(domains_to_include).intersection(set(ideo_df[ideo_df[\"mbfc_ideo\"]==1][\"domain\"].tolist()))\n",
    "centers = set(domains_to_include).intersection(set(ideo_df[ideo_df[\"mbfc_ideo\"]==0][\"domain\"].tolist()))\n",
    "\n",
    "print(\"# of domains:\", len(domains_to_include))\n",
    "print(\"# of right-leaning domains:\", len(rights))\n",
    "print(\"# of center domains:\", len(centers))\n",
    "print(\"# of left-leaning domains:\", len(lefts))\n",
    "print(\"# of low-credibility domains:\", len(lowcs))\n",
    "print(\"# of traditional domains:\", len(trads))\n",
    "\n",
    "# 2016\n",
    "# of domains: 443\n",
    "# of right-leaning domains: 83\n",
    "# of center domains: 115\n",
    "# of left-leaning domains: 185\n",
    "# of low-credibility domains: 46\n",
    "# of traditional domains: 363"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load popularity list (uncomment this block if we want to weight by popularity)\n",
    "\n",
    "032424: the current version doesn't use popularity weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load popularity weights \n",
    "# popularity_df = pd.read_csv(\"../../index/domains/domain_popularity.csv\")\n",
    "# popularity_df\n",
    "\n",
    "# # create a popularity dict {domain:weight}\n",
    "popularity_dict = {}\n",
    "\n",
    "# for _,row in popularity_df.iterrows():\n",
    "#     popularity_dict[row[\"domain\"]] = row[\"ave_m_log10\"]\n",
    "#     popularity_dict[row[\"domain\"]] = row[\"ave_m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating: \n",
      "\t# of unique domains: 743\n",
      "\tstart: 2020-07-01 00:00:00\n",
      "\tend: 2020-11-29 00:00:00\n",
      "\t# of unique domains: 786\n",
      "\tstart: 2020-07-01 00:00:00\n",
      "\tend: 2020-11-29 00:00:00\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 152, 1453)\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 152, 1453)\n",
      "Aggregating: _lowc\n",
      "\t# of unique domains: 189\n",
      "\tstart: 2020-07-01 00:00:00\n",
      "\tend: 2020-11-29 00:00:00\n",
      "\t# of unique domains: 211\n",
      "\tstart: 2020-07-01 00:00:00\n",
      "\tend: 2020-11-29 00:00:00\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 152, 1453)\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 152, 1453)\n",
      "Aggregating: _trad\n",
      "\t# of unique domains: 481\n",
      "\tstart: 2020-07-01 00:00:00\n",
      "\tend: 2020-11-29 00:00:00\n",
      "\t# of unique domains: 496\n",
      "\tstart: 2020-07-01 00:00:00\n",
      "\tend: 2020-11-29 00:00:00\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 152, 1453)\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 152, 1453)\n",
      "Aggregating: _left\n",
      "\t# of unique domains: 233\n",
      "\tstart: 2020-07-01 00:00:00\n",
      "\tend: 2020-11-29 00:00:00\n",
      "\t# of unique domains: 246\n",
      "\tstart: 2020-07-01 00:00:00\n",
      "\tend: 2020-11-29 00:00:00\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 152, 1453)\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 152, 1453)\n",
      "Aggregating: _center\n",
      "\t# of unique domains: 130\n",
      "\tstart: 2020-07-01 00:00:00\n",
      "\tend: 2020-11-29 00:00:00\n",
      "\t# of unique domains: 133\n",
      "\tstart: 2020-07-01 00:00:00\n",
      "\tend: 2020-11-29 00:00:00\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 152, 1453)\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 152, 1453)\n",
      "Aggregating: _right\n",
      "\t# of unique domains: 223\n",
      "\tstart: 2020-07-01 00:00:00\n",
      "\tend: 2020-11-29 00:00:00\n",
      "\t# of unique domains: 227\n",
      "\tstart: 2020-07-01 00:00:00\n",
      "\tend: 2020-11-29 00:00:00\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 152, 1453)\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 152, 1453)\n"
     ]
    }
   ],
   "source": [
    "SUBSET_LABEL = [\"\", \"_lowc\", \"_trad\", \"_left\", \"_center\", \"_right\"]\n",
    "DOMAIN_LIST = [domains_to_include, lowcs, trads, lefts, centers, rights]\n",
    "frac = .8\n",
    "\n",
    "weight_by_popularity = False\n",
    "normalize_by_snapshot = True\n",
    "\n",
    "OUTPUT_FOLDER = \"headline-filter0.5-nopopw-normsnap\"\n",
    "if not os.path.exists(OUTPUT_FPATH + OUTPUT_FOLDER): \n",
    "    os.mkdir(OUTPUT_FPATH + OUTPUT_FOLDER)\n",
    "\n",
    "if not os.path.exists(OUTPUT_FPATH + OUTPUT_FOLDER + \"/bootstrap\"): \n",
    "    os.mkdir(OUTPUT_FPATH + OUTPUT_FOLDER + \"/bootstrap\")\n",
    "\n",
    "for this_subset, this_list in zip(SUBSET_LABEL, DOMAIN_LIST):\n",
    "    print(\"Aggregating:\", this_subset)\n",
    "    aggr_headline1 = aggregate_headline_wordvec(\n",
    "        output_df=headline_words1, raw_df=headlines.df_cand1, aggr_unit=AGGR_UNIT,\n",
    "        dictionary=dictionary, select_domains=this_list, print_info=True, \n",
    "        weight_by_popularity=weight_by_popularity, popularity_dict=popularity_dict,\n",
    "        normalize_by_snapshot=normalize_by_snapshot)\n",
    "    aggr_headline2 = aggregate_headline_wordvec(\n",
    "        output_df=headline_words2, raw_df=headlines.df_cand2, aggr_unit=AGGR_UNIT,\n",
    "        dictionary=dictionary, select_domains=this_list, print_info=True,\n",
    "        weight_by_popularity=weight_by_popularity, popularity_dict=popularity_dict,\n",
    "        normalize_by_snapshot=normalize_by_snapshot)\n",
    "    \n",
    "    bstr_headline_arr1 = bootstrap_aggregate_wordvec(\n",
    "        data_source=\"headline\", output_df=headline_words1, aggr_unit=AGGR_UNIT,\n",
    "        dictionary=dictionary, raw_df=headlines.df_cand1, select_domains=this_list, \n",
    "        force_time_window=aggr_headline1[\"date\"].tolist(), sample_frac=frac, \n",
    "        weight_by_popularity=weight_by_popularity, popularity_dict=popularity_dict,\n",
    "        normalize_by_snapshot=normalize_by_snapshot)\n",
    "    bstr_headline_arr2 = bootstrap_aggregate_wordvec(\n",
    "        data_source=\"headline\", output_df=headline_words2, aggr_unit=AGGR_UNIT,\n",
    "        dictionary=dictionary, raw_df=headlines.df_cand2, select_domains=this_list, \n",
    "        force_time_window=aggr_headline2[\"date\"].tolist(), sample_frac=frac,\n",
    "        weight_by_popularity=weight_by_popularity, popularity_dict=popularity_dict,\n",
    "        normalize_by_snapshot=normalize_by_snapshot)\n",
    "    \n",
    "    aggr_headline1.to_pickle(f\"{OUTPUT_FPATH}/{OUTPUT_FOLDER}/{cand1}{year}_wordvecs{this_subset}.pkl\")\n",
    "    np.save(f\"{OUTPUT_FPATH}/{OUTPUT_FOLDER}/bootstrap/{cand1}{year}_bstr_wordvecs{this_subset}.npy\", bstr_headline_arr1)\n",
    "\n",
    "    aggr_headline2.to_pickle(f\"{OUTPUT_FPATH}/{OUTPUT_FOLDER}/{cand2}{year}_wordvecs{this_subset}.pkl\")\n",
    "    np.save(f\"{OUTPUT_FPATH}/{OUTPUT_FOLDER}/bootstrap/{cand2}{year}_bstr_wordvecs{this_subset}.npy\", bstr_headline_arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ceren2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
