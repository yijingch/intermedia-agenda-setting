{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/yijingch/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/yijingch/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_FPATH==/Users/yijingch/Documents/GITHUB/intermedia-agenda-setting/output/date-topvec-min2-gtm1/\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.utils.dict_loader import TopicDictionary\n",
    "from src.utils.output_loader import load_all_topvecs, load_all_bstr_arrs\n",
    "from src.utils.downstream_aggregate import normalize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n",
    "\n",
    "import yaml\n",
    "with open(\"../../src/configs.yml\", \"r\") as configs:\n",
    "    configs = yaml.safe_load(configs)\n",
    "\n",
    "ROOTPATH = configs[\"ROOTPATH\"]\n",
    "\n",
    "START2016 = pd.to_datetime(configs[\"START2016\"])\n",
    "END2016 = pd.to_datetime(configs[\"END2016\"])\n",
    "START2020 = pd.to_datetime(configs[\"START2020\"])\n",
    "END2020 = pd.to_datetime(configs[\"END2020\"])\n",
    "\n",
    "INPUT_FPATH = configs[\"DATE_TOPVEC_PATH\"]\n",
    "print(f\"INPUT_FPATH=={INPUT_FPATH}\")\n",
    "\n",
    "# OUTPUT_FPATH = ROOTPATH + \"output/figs/\"\n",
    "# print(f\"OUTPUT_FPATH=={OUTPUT_FPATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dictionary!\n",
      "\t# of unique topics: 27\n",
      "\t# of unique words: 1426\n",
      "Successfully loaded dictionary!\n",
      "\t# of unique topics: 27\n",
      "\t# of unique words: 1453\n"
     ]
    }
   ],
   "source": [
    "from src.utils.dict_configuration import dictionary2016, dictionary2020\n",
    "\n",
    "TOPICS2DROP = [\n",
    "    \"election_campaign\", \"general_controversies\", \"no_topic\", \n",
    "    \"forestry\", \"land_water_management\", \"agriculture\", \"housing\", \n",
    "    \"transportation\", \"culture\"]\n",
    "\n",
    "TOPICS2DROP_IDX2016 = [dictionary2016.topic2index[x] for x in TOPICS2DROP]\n",
    "TOPICS2DROP_IDX2020 = [dictionary2020.topic2index[x] for x in TOPICS2DROP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topvec_dfs2016 = load_all_topvecs(year=2016, topvec_fpath=INPUT_FPATH, normalize_by_unit=True)\n",
    "topvec_dfs2020 = load_all_topvecs(year=2020, topvec_fpath=INPUT_FPATH, normalize_by_unit=True)\n",
    "\n",
    "bstr_arrs2016 = load_all_bstr_arrs(year=2016, vec_fpath=INPUT_FPATH, vec_type=\"topvecs\", normalize_by_unit=True)\n",
    "bstr_arrs2020 = load_all_bstr_arrs(year=2020, vec_fpath=INPUT_FPATH, vec_type=\"topvecs\", normalize_by_unit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from collections import Counter\n",
    "\n",
    "# https://rishi-a.github.io/2020/05/25/granger-causality.html\n",
    "\n",
    "def granger_causality_all_topics(arr1, arr2, dictionary, maxlag=5, topics2skip=TOPICS2DROP, verbose=False, p_thres=.01, postfix=\"\"):\n",
    "    arr1 = np.diff(arr1, axis=0)\n",
    "    arr2 = np.diff(arr2, axis=0)\n",
    "\n",
    "    topics2test = list(set(dictionary.topics) - set(TOPICS2DROP))\n",
    "    out_df = pd.DataFrame()\n",
    "    out_df[\"topic\"] = topics2test \n",
    "    p_values = []\n",
    "\n",
    "    for topic in topics2test:\n",
    "        topic_idx = dictionary.topic2index[topic]\n",
    "        test_df = pd.DataFrame()\n",
    "        test_df[\"ts_Y\"] = arr1[:,topic_idx]\n",
    "        test_df[\"ts_X\"] = arr2[:,topic_idx]\n",
    "        # whether the time series in the second column Granger causes the time series in the first column\n",
    "        if test_df[\"ts_X\"].nunique() > 1 and test_df[\"ts_Y\"].nunique() > 1:\n",
    "            # print(test_df)\n",
    "            try:\n",
    "                test_result = grangercausalitytests(test_df, maxlag=maxlag, verbose=False)\n",
    "                p = [round(test_result[i+1][0][\"ssr_chi2test\"][1],4) for i in range(maxlag)]\n",
    "                # p = [round(test_result[i+1][0][\"ssr_ftest\"][1],4) for i in range(maxlag)]\n",
    "                if verbose: print(f\"topic = {topic}, p = {p}\")\n",
    "            except Exception as e:\n",
    "                # print(topic, e) \n",
    "                # print(topic)\n",
    "                p = [np.NaN for i in range(maxlag)]\n",
    "        else:\n",
    "            p = [np.NaN for i in range(maxlag)]\n",
    "        p_values.append(p)\n",
    "\n",
    "    out_df[f\"p_value{postfix}\"] = p_values\n",
    "    out_df[f\"opt_lag{postfix}\"] = out_df[f\"p_value{postfix}\"].map(lambda x: np.where(np.array(x) == np.array(x).min())[0]+1)\n",
    "    out_df[f\"opt_lag_sig{p_thres}{postfix}\"] = out_df.apply(lambda x: [y for y in x[f\"opt_lag{postfix}\"] if x[f\"p_value{postfix}\"][y-1]<p_thres], axis=1)\n",
    "    return out_df\n",
    "\n",
    "def granger_causality_for_df(df1, df2, dictionary, maxlag=5, topics2skip=TOPICS2DROP, verbose=False, p_thres=.01):\n",
    "    arr1 = np.array(df1[\"majority_topvec\"].tolist())\n",
    "    arr2 = np.array(df2[\"majority_topvec\"].tolist())\n",
    "    out_df = granger_causality_all_topics(\n",
    "        arr1, arr2, \n",
    "        dictionary=dictionary, \n",
    "        maxlag=maxlag, \n",
    "        topics2skip=topics2skip, \n",
    "        verbose=verbose, \n",
    "        p_thres=p_thres)\n",
    "    return out_df\n",
    "\n",
    "def granger_causality_for_bstr_arrs(bstr_arrs1, bstr_arrs2, dictionary, maxlag=5, topics2skip=TOPICS2DROP, verbose=False, p_thres=.01):\n",
    "    n_runs = bstr_arrs1.shape[0]\n",
    "    big_out_df = pd.DataFrame()\n",
    "    for r in range(n_runs):\n",
    "        out_df = granger_causality_all_topics(\n",
    "            bstr_arrs1[r], bstr_arrs2[r],\n",
    "            dictionary=dictionary, maxlag=maxlag, \n",
    "            topics2skip=topics2skip, verbose=verbose, p_thres=p_thres, postfix=str(r))\n",
    "        big_out_df = pd.concat([big_out_df, out_df], axis=1)\n",
    "        if r == 0:\n",
    "            topics = out_df[\"topic\"].tolist()\n",
    "    big_out_df.drop(columns=\"topic\", inplace=True)\n",
    "    big_out_df[\"topic\"] = topics\n",
    "    big_out_df[f\"ls_opt_lag_sig{p_thres}\"] = big_out_df.apply(lambda x: [i for ls in [x[f\"opt_lag_sig{p_thres}{r}\"] for r in range(200)] for i in ls], axis=1)\n",
    "    big_out_df[f\"count_opt_lag_sig{p_thres}\"] = big_out_df[f\"ls_opt_lag_sig{p_thres}\"].map(lambda x: Counter(x))\n",
    "    for lag in range(maxlag):\n",
    "        big_out_df[f\"lag{lag+1}_count{p_thres}\"] = big_out_df[f\"count_opt_lag_sig{p_thres}\"].map(lambda x: x[lag+1] if lag+1 in x.keys() else 0)\n",
    "    return big_out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Granger causality for sliding windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRES = 0.05\n",
    "\n",
    "def generate_output_df(dictionary, labela, labelb, a2b_cand, b2a_cand, thres=THRES):\n",
    "    output_df = pd.DataFrame() \n",
    "    output_df[\"topic\"] = dictionary.topics\n",
    "\n",
    "    output_df = output_df.merge(a2b_cand[[\"topic\",f\"opt_lag_sig{thres}\"]])\n",
    "    output_df[f\"led_by_{labela}\"] = output_df[f\"opt_lag_sig{thres}\"].map(lambda x: 1 if len(x) > 0 else 0)\n",
    "    output_df.drop(columns=f\"opt_lag_sig{thres}\", inplace=True)\n",
    "\n",
    "    output_df = output_df.merge(b2a_cand[[\"topic\",f\"opt_lag_sig{thres}\"]])\n",
    "    output_df[f\"led_by_{labelb}\"] = output_df[f\"opt_lag_sig{thres}\"].map(lambda x: 1 if len(x) > 0 else 0)\n",
    "    output_df.drop(columns=f\"opt_lag_sig{thres}\", inplace=True)\n",
    "\n",
    "    output_df[\"mutual\"] = output_df.apply(lambda x: 1 if x[f\"led_by_{labela}\"] + x[f\"led_by_{labelb}\"] == 2 else 0, axis=1)\n",
    "    output_df[\"no_relation\"] = output_df.apply(lambda x: 1 if x[f\"led_by_{labela}\"] + x[f\"led_by_{labelb}\"] == 0 else 0, axis=1)\n",
    "\n",
    "    output_df[f\"led_by_{labela}_only\"] = output_df[f\"led_by_{labela}\"] - output_df[\"mutual\"]\n",
    "    output_df[f\"led_by_{labelb}_only\"] = output_df[f\"led_by_{labelb}\"] - output_df[\"mutual\"]\n",
    "\n",
    "    return output_df[[\"topic\", f\"led_by_{labela}_only\", f\"led_by_{labelb}_only\", \"mutual\", \"no_relation\"]]\n",
    "\n",
    "def generate_output_df_bstr(dictionary, labela, labelb, bstr_a2b_cand, bstr_b2a_cand, thres=THRES):\n",
    "    output_df_all = pd.DataFrame()\n",
    "    output_df_all[\"topic\"] = dictionary.topics\n",
    "    nruns = 200\n",
    "    for r in range(nruns):\n",
    "        a2b_cand = bstr_a2b_cand[[\"topic\",f\"opt_lag_sig{thres}{r}\"]].copy()\n",
    "        a2b_cand.rename(columns={f\"opt_lag_sig{thres}{r}\": f\"opt_lag_sig{thres}\"}, inplace=True)\n",
    "        b2a_cand = bstr_b2a_cand[[\"topic\",f\"opt_lag_sig{thres}{r}\"]].copy()\n",
    "        b2a_cand.rename(columns={f\"opt_lag_sig{thres}{r}\": f\"opt_lag_sig{thres}\"}, inplace=True)\n",
    "        output_df_r = generate_output_df(dictionary, labela, labelb, a2b_cand, b2a_cand, thres=thres)\n",
    "        output_df_r.rename(columns={\n",
    "            f\"led_by_{labela}_only\": f\"led_by_{labela}_only_{r}\",\n",
    "            f\"led_by_{labelb}_only\": f\"led_by_{labelb}_only_{r}\",\n",
    "            \"mutual\": f\"mutual_{r}\",\n",
    "            \"no_relation\": f\"no_relation_{r}\"}, inplace=True)\n",
    "        output_df_all = pd.concat([output_df_all, output_df_r], axis=1)\n",
    "        if r != nruns-1:\n",
    "            output_df_all.drop(columns=\"topic\", inplace=True)\n",
    "\n",
    "    output_df_all[f\"led_by_{labela}_only\"] = output_df_all.apply(\n",
    "        lambda x: np.sum([x[f\"led_by_{labela}_only_{r}\"] for r in range(nruns)]), axis=1)\n",
    "    output_df_all[f\"led_by_{labelb}_only\"] = output_df_all.apply(\n",
    "        lambda x: np.sum([x[f\"led_by_{labelb}_only_{r}\"] for r in range(nruns)]), axis=1)\n",
    "    output_df_all[\"mutual\"] = output_df_all.apply(\n",
    "        lambda x: np.sum([x[f\"mutual_{r}\"] for r in range(nruns)]), axis=1)\n",
    "    output_df_all[\"no_relation\"] = output_df_all.apply(\n",
    "        lambda x: np.sum([x[f\"no_relation_{r}\"] for r in range(nruns)]), axis=1)\n",
    "\n",
    "    cols = [\"topic\", f\"led_by_{labela}_only\", f\"led_by_{labelb}_only\", \"mutual\", \"no_relation\"]\n",
    "\n",
    "    return output_df_all[cols].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 0\n",
      "window: 1\n",
      "window: 2\n",
      "window: 3\n",
      "window: 4\n",
      "window: 5\n",
      "window: 6\n",
      "window: 7\n",
      "window: 8\n",
      "window: 9\n",
      "window: 10\n",
      "window: 11\n",
      "window: 12\n",
      "window: 13\n",
      "window: 14\n",
      "window: 15\n",
      "window: 16\n",
      "window: 17\n",
      "window: 18\n",
      "window: 19\n",
      "window: 20\n",
      "window: 21\n",
      "window: 22\n",
      "window: 23\n",
      "window: 24\n",
      "window: 25\n",
      "window: 26\n",
      "window: 27\n",
      "window: 28\n",
      "window: 29\n",
      "window: 30\n",
      "window: 31\n",
      "window: 32\n",
      "window: 33\n",
      "window: 34\n",
      "window: 35\n",
      "window: 36\n",
      "window: 37\n",
      "window: 38\n",
      "window: 39\n",
      "window: 40\n",
      "window: 41\n",
      "window: 42\n",
      "window: 43\n",
      "window: 44\n",
      "window: 45\n",
      "window: 46\n",
      "window: 47\n",
      "window: 48\n",
      "window: 49\n",
      "window: 50\n",
      "window: 51\n",
      "window: 52\n",
      "window: 53\n",
      "window: 54\n",
      "window: 55\n",
      "window: 56\n",
      "window: 57\n",
      "window: 58\n",
      "window: 59\n",
      "window: 60\n",
      "window: 61\n"
     ]
    }
   ],
   "source": [
    "year = 2020\n",
    "if year == 2016:\n",
    "    cand1 = \"trump\"\n",
    "    cand2 = \"clinton\"\n",
    "    dictionary = dictionary2016\n",
    "    topvec_dfs = topvec_dfs2016\n",
    "    bstr_arrs = bstr_arrs2016\n",
    "elif year == 2020:\n",
    "    cand1 = \"biden\"\n",
    "    cand2 = \"trump\"\n",
    "    dictionary = dictionary2020\n",
    "    topvec_dfs = topvec_dfs2020\n",
    "    bstr_arrs = bstr_arrs2020\n",
    "\n",
    "THRES = 0.05\n",
    "WINDOW_LEN = 90\n",
    "SLIDING_WINDOW_OUTPUT = []\n",
    "\n",
    "labela = \"trad\"\n",
    "labelb = \"lowc\"\n",
    "\n",
    "for i in range(len(topvec_dfs[\"headline\"][0][1])-WINDOW_LEN):\n",
    "\n",
    "    print(\"window:\", i)\n",
    "    trad2lowc_cand1 = granger_causality_for_df(topvec_dfs[\"headline\"][0][1].loc[i:i+WINDOW_LEN], topvec_dfs[\"headline\"][0][2].loc[i:i+WINDOW_LEN], dictionary, p_thres=THRES)\n",
    "    lowc2trad_cand1 = granger_causality_for_df(topvec_dfs[\"headline\"][0][2].loc[i:i+WINDOW_LEN], topvec_dfs[\"headline\"][0][1].loc[i:i+WINDOW_LEN], dictionary, p_thres=THRES)\n",
    "\n",
    "    trad2lowc_cand2 = granger_causality_for_df(topvec_dfs[\"headline\"][1][1].loc[i:i+WINDOW_LEN], topvec_dfs[\"headline\"][1][2].loc[i:i+WINDOW_LEN], dictionary, p_thres=THRES)\n",
    "    lowc2trad_cand2 = granger_causality_for_df(topvec_dfs[\"headline\"][1][2].loc[i:i+WINDOW_LEN], topvec_dfs[\"headline\"][1][1].loc[i:i+WINDOW_LEN], dictionary, p_thres=THRES)\n",
    "\n",
    "    bstr_trad2lowc_cand1 = granger_causality_for_bstr_arrs(bstr_arrs[\"headline\"][0][1][:,i:i+WINDOW_LEN,:], bstr_arrs[\"headline\"][0][2][:,i:i+WINDOW_LEN,], dictionary, p_thres=THRES)\n",
    "    bstr_lowc2trad_cand1 = granger_causality_for_bstr_arrs(bstr_arrs[\"headline\"][0][2][:,i:i+WINDOW_LEN,:], bstr_arrs[\"headline\"][0][1][:,i:i+WINDOW_LEN,], dictionary, p_thres=THRES)\n",
    "\n",
    "    bstr_trad2lowc_cand2 = granger_causality_for_bstr_arrs(bstr_arrs[\"headline\"][1][1][:,i:i+WINDOW_LEN,:], bstr_arrs[\"headline\"][1][2][:,i:i+WINDOW_LEN,], dictionary, p_thres=THRES)\n",
    "    bstr_lowc2trad_cand2 = granger_causality_for_bstr_arrs(bstr_arrs[\"headline\"][1][2][:,i:i+WINDOW_LEN,:], bstr_arrs[\"headline\"][1][1][:,i:i+WINDOW_LEN,], dictionary, p_thres=THRES)\n",
    "\n",
    "    cred1_output = generate_output_df(dictionary, labela=labela, labelb=labelb,\n",
    "        a2b_cand=trad2lowc_cand1, b2a_cand=lowc2trad_cand1, thres=THRES)\n",
    "\n",
    "    cred2_output = generate_output_df(dictionary, labela=labela, labelb=labelb,\n",
    "        a2b_cand=trad2lowc_cand2, b2a_cand=lowc2trad_cand2, thres=THRES)\n",
    "\n",
    "    bstr_cred1_output = generate_output_df_bstr(\n",
    "        dictionary, labela=labela, labelb=labelb,\n",
    "        bstr_a2b_cand=bstr_trad2lowc_cand1, bstr_b2a_cand=bstr_lowc2trad_cand1, thres=THRES)\n",
    "\n",
    "    bstr_cred2_output = generate_output_df_bstr(\n",
    "        dictionary, labela=labela, labelb=labelb,\n",
    "        bstr_a2b_cand=bstr_trad2lowc_cand2, bstr_b2a_cand=bstr_lowc2trad_cand2, thres=THRES)\n",
    "\n",
    "    bstr_cred1_output.rename(columns={\n",
    "        f\"led_by_{labela}_only\": f\"led_by_{labela}_only_{cand1}\",\n",
    "        f\"led_by_{labelb}_only\": f\"led_by_{labelb}_only_{cand1}\",\n",
    "        \"mutual\": f\"mutual_{cand1}\",\n",
    "        \"no_relation\": f\"no_relation_{cand1}\"}, inplace=True)\n",
    "    bstr_cred2_output.rename(columns={\n",
    "        f\"led_by_{labela}_only\": f\"led_by_{labela}_only_{cand2}\",\n",
    "        f\"led_by_{labelb}_only\": f\"led_by_{labelb}_only_{cand2}\",\n",
    "        \"mutual\": f\"mutual_{cand2}\",\n",
    "        \"no_relation\": f\"no_relation_{cand2}\"}, inplace=True)\n",
    "\n",
    "    SLIDING_WINDOW_OUTPUT.append([bstr_cred1_output, bstr_cred2_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labela = \"left\"\n",
    "# labelb = \"right\"\n",
    "\n",
    "# for i in range(len(topvec_dfs[\"headline\"][0][1])-WINDOW_LEN):\n",
    "\n",
    "#     print(\"window:\", i)\n",
    "#     left2right_cand1 = granger_causality_for_df(topvec_dfs[\"headline\"][0][5].loc[i:i+WINDOW_LEN], topvec_dfs[\"headline\"][0][3].loc[i:i+WINDOW_LEN], dictionary, p_thres=THRES)\n",
    "#     right2left_cand1 = granger_causality_for_df(topvec_dfs[\"headline\"][0][3].loc[i:i+WINDOW_LEN], topvec_dfs[\"headline\"][0][5].loc[i:i+WINDOW_LEN], dictionary, p_thres=THRES)\n",
    "\n",
    "#     left2right_cand2 = granger_causality_for_df(topvec_dfs[\"headline\"][1][5].loc[i:i+WINDOW_LEN], topvec_dfs[\"headline\"][1][3].loc[i:i+WINDOW_LEN], dictionary, p_thres=THRES)\n",
    "#     right2left_cand2 = granger_causality_for_df(topvec_dfs[\"headline\"][1][3].loc[i:i+WINDOW_LEN], topvec_dfs[\"headline\"][1][5].loc[i:i+WINDOW_LEN], dictionary, p_thres=THRES)\n",
    "\n",
    "#     bstr_left2right_cand1 = granger_causality_for_bstr_arrs(bstr_arrs[\"headline\"][0][5][:,i:i+WINDOW_LEN,:], bstr_arrs[\"headline\"][0][3][:,i:i+WINDOW_LEN,], dictionary, p_thres=THRES)\n",
    "#     bstr_right2left_cand1 = granger_causality_for_bstr_arrs(bstr_arrs[\"headline\"][0][3][:,i:i+WINDOW_LEN,:], bstr_arrs[\"headline\"][0][5][:,i:i+WINDOW_LEN,], dictionary, p_thres=THRES)\n",
    "\n",
    "#     bstr_left2right_cand2 = granger_causality_for_bstr_arrs(bstr_arrs[\"headline\"][1][5][:,i:i+WINDOW_LEN,:], bstr_arrs[\"headline\"][1][3][:,i:i+WINDOW_LEN,], dictionary, p_thres=THRES)\n",
    "#     bstr_right2left_cand2 = granger_causality_for_bstr_arrs(bstr_arrs[\"headline\"][1][3][:,i:i+WINDOW_LEN,:], bstr_arrs[\"headline\"][1][5][:,i:i+WINDOW_LEN,], dictionary, p_thres=THRES)\n",
    "\n",
    "#     ideo1_output = generate_output_df(dictionary, labela=labela, labelb=labelb,\n",
    "#         a2b_cand=left2right_cand1, b2a_cand=right2left_cand1, thres=THRES)\n",
    "\n",
    "#     ideo2_output = generate_output_df(dictionary, labela=labela, labelb=labelb,\n",
    "#         a2b_cand=left2right_cand2, b2a_cand=right2left_cand2, thres=THRES)\n",
    "\n",
    "#     bstr_ideo1_output = generate_output_df_bstr(\n",
    "#         dictionary, labela=labela, labelb=labelb,\n",
    "#         bstr_a2b_cand=bstr_left2right_cand1, bstr_b2a_cand=bstr_right2left_cand1, thres=THRES)\n",
    "\n",
    "#     bstr_ideo2_output = generate_output_df_bstr(\n",
    "#         dictionary, labela=labela, labelb=labelb,\n",
    "#         bstr_a2b_cand=bstr_left2right_cand2, bstr_b2a_cand=bstr_right2left_cand2, thres=THRES)\n",
    "\n",
    "#     bstr_ideo1_output.rename(columns={\n",
    "#         f\"led_by_{labela}_only\": f\"led_by_{labela}_only_{cand1}\",\n",
    "#         f\"led_by_{labelb}_only\": f\"led_by_{labelb}_only_{cand1}\",\n",
    "#         \"mutual\": f\"mutual_{cand1}\",\n",
    "#         \"no_relation\": f\"no_relation_{cand1}\"}, inplace=True)\n",
    "#     bstr_ideo2_output.rename(columns={\n",
    "#         f\"led_by_{labela}_only\": f\"led_by_{labela}_only_{cand2}\",\n",
    "#         f\"led_by_{labelb}_only\": f\"led_by_{labelb}_only_{cand2}\",\n",
    "#         \"mutual\": f\"mutual_{cand2}\",\n",
    "#         \"no_relation\": f\"no_relation_{cand2}\"}, inplace=True)\n",
    "\n",
    "#     SLIDING_WINDOW_OUTPUT.append([bstr_ideo1_output, bstr_ideo2_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBUST_THRES = 0.95*200\n",
    "def get_significant_result(bstr_group_output, thres=ROBUST_THRES):\n",
    "    cols = list(bstr_group_output.columns)\n",
    "    cols.remove(\"topic\")\n",
    "    sig_result = {}\n",
    "    for _,row in bstr_group_output.iterrows():\n",
    "        t = row[\"topic\"]\n",
    "        largest = np.max([row[c] for c in cols])\n",
    "        sig_result[t] = \"NO_SIG_RESULT\"\n",
    "        for c in cols:\n",
    "            if row[c] == largest and row[c] > thres:\n",
    "                sig_result[t] = c\n",
    "    return sig_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODES = [f\"led_by_{labela}_only\", f\"led_by_{labelb}_only\", \"mutual\", \"no_relation\"]\n",
    "tmp_plot_ias = pd.DataFrame()\n",
    "tmp_plot_ias[\"date\"] = topvec_dfs[\"headline\"][0][1][:len(SLIDING_WINDOW_OUTPUT)][\"date\"]\n",
    "res_cand1 = []\n",
    "res_cand2 = []\n",
    "for bstr_cred1_output, bstr_cred2_output in SLIDING_WINDOW_OUTPUT:\n",
    "    res_cand1.append(get_significant_result(bstr_cred1_output))\n",
    "    res_cand2.append(get_significant_result(bstr_cred2_output))\n",
    "# for bstr_ideo1_output, bstr_ideo2_output in SLIDING_WINDOW_OUTPUT:\n",
    "#     res_cand1.append(get_significant_result(bstr_ideo1_output))\n",
    "#     res_cand2.append(get_significant_result(bstr_ideo2_output))\n",
    "tmp_plot_ias[f\"res_{cand1}\"] = res_cand1\n",
    "tmp_plot_ias[f\"res_{cand2}\"] = res_cand2\n",
    "\n",
    "for t in tmp_plot_ias[f\"res_{cand1}\"].tolist()[0].keys():\n",
    "    tmp_plot_ias[f\"{t}_{cand1}\"] = tmp_plot_ias[f\"res_{cand1}\"].map(lambda x: x[t])\n",
    "    tmp_plot_ias[f\"{t}_{cand2}\"] = tmp_plot_ias[f\"res_{cand2}\"].map(lambda x: x[t])\n",
    "tmp_plot_ias.to_csv(ROOTPATH + f\"output/ias-sliding-window/cred_{year}_window90.csv\", index=False)\n",
    "# tmp_plot_ias.to_csv(ROOTPATH + f\"output/ias-sliding-window/ideo_{year}_window90.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ceren-clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
