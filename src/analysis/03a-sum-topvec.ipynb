{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For aggregated analysis at the topic level, we sum up the raw count of topics without aggregating first by a certain time unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/yijingch/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/yijingch/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os, re\n",
    "from datetime import datetime\n",
    "\n",
    "from src.utils.downstream_aggregate import load_model_output \n",
    "from src.utils.downstream_sum import sum_headline_topvec, sum_survey_topvec, sum_tweet_topvec\n",
    "from src.utils.downstream_sum import bootstrap_sum_topvec\n",
    "from src.utils.data_loader import Headlines, Surveys, Tweets\n",
    "from src.utils.downstream_process import clean_domain_url, trim_period\n",
    "from src.utils.dict_loader import TopicDictionary\n",
    "\n",
    "import yaml\n",
    "with open(\"../../src/configs.yml\", \"r\") as configs:\n",
    "    configs = yaml.safe_load(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input \n",
    "ROOTPATH = configs[\"ROOTPATH\"]\n",
    "DATAPATH = configs[\"DATAPATH\"] \n",
    "CACHE_FPATH = configs[\"TOPVEC_CACHE_PATH\"]\n",
    "date_string = \"032424\"\n",
    "\n",
    "# output\n",
    "OUTPUT_FPATH = configs[\"SUM_TOPVEC_PATH\"]\n",
    "\n",
    "if not os.path.exists(OUTPUT_FPATH):\n",
    "    os.mkdir(OUTPUT_FPATH)\n",
    "    print(\"Created output folder!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dictionary!\n",
      "\t# of unique topics: 27\n",
      "\t# of unique words: 1426\n",
      "Successfully loaded dictionary!\n",
      "\t# of unique topics: 27\n",
      "\t# of unique words: 1453\n",
      "2020-07-01 00:00:00\n",
      "2020-11-30 00:00:00\n",
      "# of domains to include: 804\n"
     ]
    }
   ],
   "source": [
    "from src.utils.dict_configuration import dictionary2016, dictionary2020\n",
    "year = 2020\n",
    "\n",
    "if year == 2016:\n",
    "    cand1 = \"trump\"\n",
    "    cand2 = \"clinton\"\n",
    "    start = pd.to_datetime(configs[\"START2016\"])\n",
    "    end = pd.to_datetime(configs[\"END2016\"])\n",
    "    dictionary = dictionary2016\n",
    "    domains_to_include_df = pd.read_csv(\"../../index/domains/domains_to_keep2016_coverage0.5.csv\")\n",
    "    \n",
    "else:\n",
    "    cand1 = \"biden\"\n",
    "    cand2 = \"trump\"\n",
    "    start = pd.to_datetime(configs[\"START2020\"])\n",
    "    end = pd.to_datetime(configs[\"END2020\"])\n",
    "    dictionary = dictionary2020\n",
    "    domains_to_include_df = pd.read_csv(\"../../index/domains/domains_to_keep2020_coverage0.5.csv\")\n",
    "\n",
    "print(start)\n",
    "print(end)\n",
    "print(\"# of domains to include:\", len(domains_to_include_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregate headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_cachepath1 = CACHE_FPATH + f\"headline/{date_string}_{cand1}{year}_topvec_cache.pkl\"\n",
    "headline_cachepath2 = CACHE_FPATH + f\"headline/{date_string}_{cand2}{year}_topvec_cache.pkl\"\n",
    "headline_topics1 = load_model_output(headline_cachepath1, start=start, end=end)\n",
    "headline_topics2 = load_model_output(headline_cachepath2, start=start, end=end)\n",
    "\n",
    "headlines = Headlines(ROOTPATH + \"data/\", year=year, drop_duplicates=False)\n",
    "headlines.trim(start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>date</th>\n",
       "      <th>n_coverage</th>\n",
       "      <th>pct_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12news.com</td>\n",
       "      <td>{Timestamp('2020-08-12 00:00:00'), Timestamp('...</td>\n",
       "      <td>145</td>\n",
       "      <td>0.953947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21stcenturywire.com</td>\n",
       "      <td>{Timestamp('2020-08-12 00:00:00'), Timestamp('...</td>\n",
       "      <td>128</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4threvolutionarywar.wordpress.com</td>\n",
       "      <td>{Timestamp('2020-08-12 00:00:00'), Timestamp('...</td>\n",
       "      <td>131</td>\n",
       "      <td>0.861842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>680news.com</td>\n",
       "      <td>{Timestamp('2020-08-12 00:00:00'), Timestamp('...</td>\n",
       "      <td>108</td>\n",
       "      <td>0.710526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abc12.com</td>\n",
       "      <td>{Timestamp('2020-08-17 00:00:00'), Timestamp('...</td>\n",
       "      <td>89</td>\n",
       "      <td>0.585526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              domain  \\\n",
       "0                         12news.com   \n",
       "1                21stcenturywire.com   \n",
       "2  4threvolutionarywar.wordpress.com   \n",
       "3                        680news.com   \n",
       "4                          abc12.com   \n",
       "\n",
       "                                                date  n_coverage  pct_coverage  \n",
       "0  {Timestamp('2020-08-12 00:00:00'), Timestamp('...         145      0.953947  \n",
       "1  {Timestamp('2020-08-12 00:00:00'), Timestamp('...         128      0.842105  \n",
       "2  {Timestamp('2020-08-12 00:00:00'), Timestamp('...         131      0.861842  \n",
       "3  {Timestamp('2020-08-12 00:00:00'), Timestamp('...         108      0.710526  \n",
       "4  {Timestamp('2020-08-17 00:00:00'), Timestamp('...          89      0.585526  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_to_include_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load domain labels --- NEW VERSION AFTER I REFRESH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df = pd.read_csv(\"../../index/domains/MASTER_fake_refreshed081123.tsv\", sep=\"\\t\")\n",
    "mixed_df = pd.read_csv(\"../../index/domains/MASTER_mixed_refreshed081123.tsv\", sep=\"\\t\")\n",
    "ideo_df = pd.read_csv(\"../../index/domains/ideo_domain_mbfc081123.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of domains: 804\n",
      "# of right-leaning domains: 232\n",
      "# of center domains: 134\n",
      "# of left-leaning domains: 249\n",
      "# of low-credibility domains: 220\n",
      "# of traditional domains: 505\n"
     ]
    }
   ],
   "source": [
    "domains_to_include = domains_to_include_df[\"domain\"].tolist()\n",
    "lowcs = set(domains_to_include).intersection(set(fake_df[fake_df[\"fake_sum\"]>=1][\"domain\"].tolist()))\n",
    "lowcs.remove(\"foxnews.com\")\n",
    "\n",
    "trads = set(domains_to_include) - set(fake_df[fake_df[\"fake_sum\"]>=1][\"domain\"].tolist())\n",
    "trads -= set(mixed_df[mixed_df[\"mixed_sum\"]>=1][\"domain\"].tolist())\n",
    "trads.add(\"foxnews.com\")\n",
    "\n",
    "lefts = set(domains_to_include).intersection(set(ideo_df[ideo_df[\"mbfc_ideo\"]==-1][\"domain\"].tolist()))\n",
    "rights = set(domains_to_include).intersection(set(ideo_df[ideo_df[\"mbfc_ideo\"]==1][\"domain\"].tolist()))\n",
    "centers = set(domains_to_include).intersection(set(ideo_df[ideo_df[\"mbfc_ideo\"]==0][\"domain\"].tolist()))\n",
    "\n",
    "print(\"# of domains:\", len(domains_to_include))\n",
    "print(\"# of right-leaning domains:\", len(rights))\n",
    "print(\"# of center domains:\", len(centers))\n",
    "print(\"# of left-leaning domains:\", len(lefts))\n",
    "print(\"# of low-credibility domains:\", len(lowcs))\n",
    "print(\"# of traditional domains:\", len(trads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load popularity list (uncomment this block if we want to weight by popularity)\n",
    "\n",
    "032424: the current version doesn't use popularity weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load popularity weights \n",
    "# popularity_df = pd.read_csv(\"../../index/domains/domain_popularity.csv\")\n",
    "# popularity_df\n",
    "\n",
    "# # create a popularity dict {domain:weight}\n",
    "popularity_dict = {}\n",
    "\n",
    "# for _,row in popularity_df.iterrows():\n",
    "#     popularity_dict[row[\"domain\"]] = row[\"ave_m_log10\"]\n",
    "#     popularity_dict[row[\"domain\"]] = row[\"ave_m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating: \n",
      "\t# of unique domains: 741\n",
      "\t# of unique domains: 784\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 27)\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 27)\n",
      "Aggregating: _lowc\n",
      "\t# of unique domains: 187\n",
      "\t# of unique domains: 210\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 27)\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 27)\n",
      "Aggregating: _trad\n",
      "\t# of unique domains: 481\n",
      "\t# of unique domains: 495\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 27)\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 27)\n",
      "Aggregating: _left\n",
      "\t# of unique domains: 233\n",
      "\t# of unique domains: 244\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 27)\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 27)\n",
      "Aggregating: _center\n",
      "\t# of unique domains: 130\n",
      "\t# of unique domains: 133\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 27)\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 27)\n",
      "Aggregating: _right\n",
      "\t# of unique domains: 222\n",
      "\t# of unique domains: 226\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 27)\n",
      "progress: 0.0\n",
      "progress: 0.1\n",
      "progress: 0.2\n",
      "progress: 0.3\n",
      "progress: 0.4\n",
      "progress: 0.5\n",
      "progress: 0.6\n",
      "progress: 0.7\n",
      "progress: 0.8\n",
      "progress: 0.9\n",
      "(200, 27)\n"
     ]
    }
   ],
   "source": [
    "SUBSET_LABEL = [\"\", \"_lowc\", \"_trad\", \"_left\", \"_center\", \"_right\"]\n",
    "DOMAIN_LIST = [domains_to_include, lowcs, trads, lefts, centers, rights]\n",
    "frac = .8\n",
    "\n",
    "weight_by_popularity = False\n",
    "normalize_by_snapshot = True\n",
    "\n",
    "\n",
    "OUTPUT_FOLDER = \"headline-filter0.5-nopopw-normsnap\"\n",
    "if not os.path.exists(OUTPUT_FPATH + OUTPUT_FOLDER): \n",
    "    os.mkdir(OUTPUT_FPATH + OUTPUT_FOLDER)\n",
    "\n",
    "if not os.path.exists(OUTPUT_FPATH + OUTPUT_FOLDER + \"/bootstrap\"): \n",
    "    os.mkdir(OUTPUT_FPATH + OUTPUT_FOLDER + \"/bootstrap\")\n",
    "\n",
    "for this_subset, this_list in zip(SUBSET_LABEL, DOMAIN_LIST):\n",
    "    print(\"Aggregating:\", this_subset)\n",
    "    sum_headline1 = sum_headline_topvec(\n",
    "        output_df=headline_topics1, raw_df=headlines.df_cand1, cand=cand1, dictionary=dictionary, select_domains=this_list, \n",
    "        weight_by_popularity=weight_by_popularity, popularity_dict=popularity_dict, \n",
    "        print_info=True, normalize_by_snapshot=normalize_by_snapshot)\n",
    "    sum_headline2 = sum_headline_topvec(\n",
    "        output_df=headline_topics2, raw_df=headlines.df_cand2, cand=cand2, dictionary=dictionary, select_domains=this_list, \n",
    "        weight_by_popularity=weight_by_popularity, popularity_dict=popularity_dict, \n",
    "        print_info=True, normalize_by_snapshot=normalize_by_snapshot)\n",
    "    \n",
    "    bstr_headline_arr1 = bootstrap_sum_topvec(\n",
    "        data_source=\"headline\", output_df=headline_topics1, cand=cand1, dictionary=dictionary, raw_df=headlines.df_cand1, \n",
    "        select_domains=this_list, weight_by_popularity=weight_by_popularity, popularity_dict=popularity_dict,\n",
    "        normalize_by_snapshot=normalize_by_snapshot, sample_frac=frac)\n",
    "    bstr_headline_arr2 = bootstrap_sum_topvec(\n",
    "        data_source=\"headline\", output_df=headline_topics2, cand=cand2, dictionary=dictionary, raw_df=headlines.df_cand2, \n",
    "        select_domains=this_list, weight_by_popularity=weight_by_popularity, popularity_dict=popularity_dict,\n",
    "        normalize_by_snapshot=normalize_by_snapshot, sample_frac=frac)\n",
    "\n",
    "    np.save(f\"{OUTPUT_FPATH}/{OUTPUT_FOLDER}/{cand1}{year}_SUM_topvecs{this_subset}.npy\", sum_headline1)\n",
    "    np.save(f\"{OUTPUT_FPATH}{OUTPUT_FOLDER}/bootstrap/{cand1}{year}_bstr_SUM_topvecs{this_subset}.npy\", bstr_headline_arr1) \n",
    "\n",
    "    np.save(f\"{OUTPUT_FPATH}/{OUTPUT_FOLDER}/{cand2}{year}_SUM_topvecs{this_subset}.npy\", sum_headline2)\n",
    "    np.save(f\"{OUTPUT_FPATH}{OUTPUT_FOLDER}/bootstrap/{cand2}{year}_bstr_SUM_topvecs{this_subset}.npy\", bstr_headline_arr2) \n",
    "\n",
    "# # this would take a while \n",
    "# # COMPLETE HEADLINE DATASET\n",
    "# # ~ 82 mins for 2020 (date 78 mins)\n",
    "# # ï½ž51 mins for 2016 (date xx mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ceren2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
